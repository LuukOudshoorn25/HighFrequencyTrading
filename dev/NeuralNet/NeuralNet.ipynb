{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from pandarallel import pandarallel\n",
    "import matplotlib.dates as mdates\n",
    "import time\n",
    "pandarallel.initialize()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.layers import Flatten, Dense, Conv1D, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickle file\n",
    "df = pd.read_pickle('../MSFT_2015_2020.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time arr for every second\n",
    "times = []\n",
    "for hours in range(9,16):\n",
    "    for minutes in range(0,60):\n",
    "        for seconds in np.arange(0,60,1):\n",
    "            h = str(hours) if hours>=10 else '0'+str(hours)\n",
    "            m = str(minutes) if minutes>=10 else '0'+str(minutes)\n",
    "            s = str(seconds) if seconds>=10 else '0'+str(seconds)\n",
    "            times.append(h+':'+m+':'+s)\n",
    "times = times[1801:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = df.resample('1D').median()\n",
    "days = days.dropna().index\n",
    "days = pd.Series(days).apply(lambda x: x.strftime('%Y-%m-%d')).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regular_df(df):\n",
    "    \"\"\"Make 1 sec regular dataframe from irregular one day dataframe of prices.\n",
    "       Note, irregular_day_df should be a 1-day only dataframe with prices\n",
    "       (no sizes here). If there is no trade, we take the previous time\"\"\"\n",
    "    # Extract times from datetimes\n",
    "    df['time'] = pd.Series(df.index.values).apply(lambda x: x.strftime('%H:%M:%S')).values\n",
    "    # Make time indes\n",
    "    df= df.set_index('time')\n",
    "    # Save irregular spaced dataframe\n",
    "    irregular_df = df.copy()\n",
    "    # Get data for all datetimes, where we fill forward if there is no data\n",
    "    df = df.reindex(times).fillna(method='ffill')\n",
    "\n",
    "    df['logprice'] = np.log(df.PRICE)\n",
    "    # Get deltalog and deltalog^2 prices\n",
    "    df['deltalog'] = df.logprice.diff()*100\n",
    "    df['deltalog2'] = (df['deltalog'])**2\n",
    "\n",
    "    irregular_df['logprice'] = np.log(irregular_df.PRICE)\n",
    "    # Get deltalog and deltalog^2 prices\n",
    "    irregular_df['deltalog'] = irregular_df.logprice.diff()*100\n",
    "    irregular_df['deltalog2'] = (irregular_df['deltalog'])**2\n",
    "    return df, irregular_df\n",
    "\n",
    "def worker(day):\n",
    "    \"\"\"worker to loop over days\"\"\"\n",
    "    daily_counts = pd.read_hdf('../days.h5')\n",
    "    counts = daily_counts.loc[day].iloc[0]\n",
    "    # to be faster: find where we are approximately\n",
    "    iloc0 = max(int(daily_counts.loc[:day].sum().iloc[0]-counts)-1000,0)\n",
    "    hdf_df = pd.read_hdf('../data.h5')\n",
    "    oneday = hdf_df.iloc[iloc0:counts+iloc0+1000].loc[day]\n",
    "    regular_df, irregular_df = make_regular_df(oneday)\n",
    "    return regular_df\n",
    "\n",
    "reg_dfs = Parallel(n_jobs=23)(delayed(worker)(i) for i in days)\n",
    "X = np.array([w.LOGPRICE.values for w in reg_dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(len(X),1,1,23399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built output data (RVOL)\n",
    "RV = (np.sqrt((np.log(df[['PRICE']].resample('5T').median()).diff()**2).resample('1D').sum())*100*np.sqrt(252))\n",
    "RV = RV[RV.PRICE>0]\n",
    "RV.columns = ['RealVol']\n",
    "Y = RV.reindex(days).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_layer = Input((1,1,23399))\n",
    "\n",
    "x = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "x = Dense(1)(x)\n",
    "output_layer = Activation('relu')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['MeanSquaredError'])\n",
    "\n",
    "model.fit(X_train\n",
    "          , y_train\n",
    "          , batch_size=16\n",
    "          , epochs=100\n",
    "          , shuffle=True\n",
    "          , validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model(X_train).numpy().flatten(), y_train)\n",
    "\n",
    "plt.scatter(model(X_test).numpy().flatten(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
